"""
App Factory Module
Creates and configures the Flask application with all necessary components.
Implements the application factory pattern for better testing and modularity.
"""

import os
import logging
import atexit
import signal
from flask import Flask, request, g
from flask_cors import CORS
from flask_socketio import SocketIO
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

from config import Config
from .api_routes import setup_routes
from .error_handlers import setup_error_handlers
from .websocket_handlers import setup_websocket_handlers
from .fallback_services import SimpleDocumentProcessor, SimpleStyleAnalyzer, SimpleAIRewriter

# Import database components
from database import init_db, db
from database.services import database_service

# Initialize monitoring system
try:
    from validation.monitoring.metrics import get_metrics
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

logger = logging.getLogger(__name__)


def create_app(config_class=Config):
    """Create and configure Flask application using the application factory pattern."""
    
    # Get the correct path for templates and static files
    # Since we're in app_modules/, we need to go up one level
    import os
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    template_dir = os.path.join(base_dir, 'ui', 'templates')
    static_dir = os.path.join(base_dir, 'ui', 'static')
    
    # Create Flask app with correct template and static paths
    app = Flask(__name__, 
                template_folder=template_dir,
                static_folder=static_dir if os.path.exists(static_dir) else None)
    app.config.from_object(config_class)
    
    # Set up logging
    setup_logging(app)
    
    # Initialize extensions
    CORS(app)
    # Use 'gevent' async mode when running with Gunicorn gevent worker
    # Use 'threading' for development (when running with Flask's built-in server)
    import os
    async_mode = 'gevent' if os.getenv('ENVIRONMENT') == 'production' else 'threading'
    
    # Configure Socket.IO with extended timeouts for long-running operations
    socketio = SocketIO(
        app, 
        cors_allowed_origins="*", 
        async_mode=async_mode,
        # Extended timeout settings for long-running analysis operations
        ping_timeout=60,        # 1 minute timeout for pong response
        ping_interval=25,       # Send ping every 25 seconds
        engineio_logger=False,  # Reduce log noise
        # Additional settings for stability
        max_http_buffer_size=10_000_000,  # 10MB for large documents
        allow_upgrades=True,
        http_compression=True
    )
    
    # Initialize rate limiter for production stability
    # Health checks are excluded to prevent pod restarts
    limiter = Limiter(
        app=app,
        key_func=get_remote_address,
        default_limits=["100 per hour", "20 per minute"],
        storage_uri="memory://",
        strategy="fixed-window"
    )
    
    app.limiter = limiter
    logger.info("âœ… Rate limiting enabled: 20 req/min per IP (health checks will be exempted)")
    
    # Initialize database
    database_initialized = initialize_database(app)
    
    # Initialize services with fallbacks
    services = initialize_services()
    
    # Add database service to services
    services['database_service'] = database_service
    services['database_available'] = database_initialized
    
    # Setup application components
    setup_routes(app, services['document_processor'], services['style_analyzer'], services['ai_rewriter'], services['database_service'])
    setup_error_handlers(app)
    setup_websocket_handlers(socketio)
    
    # Store services and socketio in app context for access
    setattr(app, 'services', services)
    setattr(app, 'socketio', socketio)
    
    # Log initialization status
    log_initialization_status(services)
    
    # Initialize monitoring system if available
    if MONITORING_AVAILABLE:
        try:
            metrics = get_metrics()
            if metrics.prometheus_enabled:
                logger.info("âœ… Prometheus metrics enabled and server started")
                logger.info("ðŸ“Š Metrics available at: http://localhost:8000/metrics")
            else:
                logger.info("ðŸ“Š Validation monitoring enabled (in-process counters only)")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to initialize monitoring: {e}")
    
    # Register cleanup handlers
    register_cleanup_handlers()
    
    return app, socketio


def setup_logging(app):
    """Configure application logging."""
    try:
        # Create logs directory if it doesn't exist
        if not os.path.exists('logs'):
            os.makedirs('logs')
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s %(levelname)s %(name)s: %(message)s',
            handlers=[
                logging.FileHandler('logs/app.log'),
                logging.StreamHandler()
            ]
        )
        
        # Set specific logger levels
        logging.getLogger('werkzeug').setLevel(logging.INFO)
        logging.getLogger('socketio').setLevel(logging.WARNING)
        logging.getLogger('engineio').setLevel(logging.WARNING)
        
        logger.info("Logging configured successfully")
        
    except Exception as e:
        print(f"Warning: Could not configure logging: {e}")


def initialize_database(app):
    """Initialize database with the Flask application."""
    try:
        # Initialize database
        init_db(app)
        
        # Create tables if they don't exist (for development)
        with app.app_context():
            db.create_all()
            
            # Initialize default rules if needed
            try:
                rules_created = database_service.initialize_default_rules()
                if rules_created > 0:
                    logger.info(f"âœ… Initialized {rules_created} default style rules")
                else:
                    logger.info("âœ… Database already contains default style rules")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not initialize default rules: {e}")
        
        logger.info("âœ… Database initialized successfully")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Database initialization failed: {e}")
        logger.warning("âš ï¸ Application will run without database persistence")
        return False

def initialize_services():
    """Initialize all services with fallback mechanisms."""
    services = {
        'document_processor': None,
        'style_analyzer': None,
        'ai_rewriter': None,
        'document_processor_available': False,
        'style_analyzer_available': False,
        'ai_rewriter_available': False
    }
    
    # âœ… PERFORMANCE OPTIMIZATION: Preload ML models once at startup
    logger.info("ðŸš€ Preloading ML models for performance optimization...")
    try:
        import spacy
        # Load spaCy model once and cache it
        _ = spacy.load("en_core_web_sm")
        logger.info("âœ… SpaCy model preloaded successfully")
    except Exception as e:
        logger.warning(f"âš ï¸ Could not preload spaCy model: {e}")
    
    # Initialize Document Processor
    try:
        from structural_parsing.extractors import DocumentProcessor
        services['document_processor'] = DocumentProcessor()
        services['document_processor_available'] = True
        logger.info("âœ… DocumentProcessor imported successfully")
    except ImportError as e:
        services['document_processor'] = SimpleDocumentProcessor()
        services['document_processor_available'] = False
        logger.warning(f"âš ï¸ Document processor not available - {e}")
    
    # Initialize Style Analyzer
    try:
        from style_analyzer import StyleAnalyzer
        services['style_analyzer'] = StyleAnalyzer()
        services['style_analyzer_available'] = True
        logger.info("âœ… StyleAnalyzer imported successfully")
    except ImportError as e:
        services['style_analyzer'] = SimpleStyleAnalyzer()
        services['style_analyzer_available'] = False
        logger.warning(f"âš ï¸ Style analyzer not available - {e}")
    
    # Initialize AI Rewriter with progress callback support
    try:
        from rewriter import DocumentRewriter
        from models import ModelConfig
        from .websocket_handlers import emit_progress
        
        # Create progress callback function for real-time updates
        def progress_callback(session_id, step, status, detail, progress_percent):
            """Progress callback that emits WebSocket updates for specific or all sessions."""
            try:
                # Use the provided session_id, or broadcast to all if empty
                # Empty string or None means broadcast to all connected clients
                target_session = session_id if session_id and session_id != 'None' else ''
                emit_progress(target_session, step, status, detail, progress_percent)
                print(f"ðŸ” DEBUG PROGRESS CALLBACK: Emitted to {'all sessions' if not target_session else target_session}")
            except Exception as e:
                logger.debug(f"Progress callback error: {e}")
                print(f"âŒ Progress callback error: {e}")
        
        # Get model configuration
        model_info = ModelConfig.get_model_info()
        
        # Initialize with progress callback for world-class real-time updates
        services['ai_rewriter'] = DocumentRewriter(progress_callback=progress_callback)
        services['ai_rewriter_available'] = True
        logger.info("âœ… DocumentRewriter imported successfully with world-class progress tracking")
        logger.info(f"AI Model: {model_info.get('type', 'Unknown')} - {model_info.get('model', 'Unknown')}")
    except ImportError as e:
        services['ai_rewriter'] = SimpleAIRewriter()
        services['ai_rewriter_available'] = False
        logger.warning(f"âš ï¸ DocumentRewriter not available, using fallback - {e}")
    
    return services


def log_initialization_status(services):
    """Log the initialization status of all services."""
    try:
        logger.info("=== Service Initialization Status ===")
        
        status_map = {
            'document_processor': 'Document Processor',
            'style_analyzer': 'Style Analyzer',
            'ai_rewriter': 'AI Rewriter',
            'database_service': 'Database Service'
        }
        
        for service_key, display_name in status_map.items():
            available_key = f"{service_key}_available"
            if service_key == 'database_service':
                # Special handling for database service
                status = "âœ… Ready" if services.get('database_available', False) else "âš ï¸ Unavailable"
            else:
                status = "âœ… Ready" if services.get(available_key, False) else "âš ï¸ Fallback"
            logger.info(f"{display_name}: {status}")
        
        # Log AI configuration with status
        try:
            from models import ModelConfig, is_model_available
            model_info = ModelConfig.get_model_info()
            active_config = ModelConfig.get_active_config()
            
            if active_config.get('provider_type') == 'ollama':
                # Check if Ollama is actually available
                ollama_status = "âœ… Ready" if is_model_available() else "âš ï¸ Connection Issue"
                model_name = active_config.get('model', 'Unknown')
                logger.info(f"AI Configuration: {ollama_status} - Ollama ({model_name})")
            elif active_config.get('provider_type') == 'api':
                api_status = "âœ… Ready" if is_model_available() else "âš ï¸ API Issue"
                provider_name = active_config.get('provider_name', 'Unknown')
                model_name = active_config.get('model', 'Unknown')
                logger.info(f"AI Configuration: {api_status} - {provider_name} API ({model_name})")
            else:
                logger.info("AI Configuration: âœ… Models system available")
        except Exception as e:
            logger.warning(f"Could not determine AI configuration: {e}")
        
        logger.info("=====================================")
        
    except Exception as e:
        logger.error(f"Error logging initialization status: {e}")


def configure_upload_folder(app):
    """Configure upload folder settings."""
    try:
        upload_folder = app.config.get('UPLOAD_FOLDER', 'uploads')
        max_content_length = app.config.get('MAX_CONTENT_LENGTH', 16 * 1024 * 1024)  # 16MB
        
        # Ensure upload directory exists
        os.makedirs(upload_folder, exist_ok=True)
        
        # Set Flask configuration
        app.config['UPLOAD_FOLDER'] = upload_folder
        app.config['MAX_CONTENT_LENGTH'] = max_content_length
        
        logger.info(f"Upload folder configured: {upload_folder} (max: {max_content_length // (1024*1024)}MB)")
        
    except Exception as e:
        logger.error(f"Error configuring upload folder: {e}")


def register_teardown_handlers(app):
    """Register application teardown handlers."""
    
    @app.teardown_appcontext
    def cleanup_session(error):
        """Cleanup session data on teardown."""
        try:
            # Add any session cleanup logic here
            pass
        except Exception as e:
            logger.error(f"Error in session cleanup: {e}")
    
    @app.teardown_request
    def cleanup_request(error):
        """Cleanup request data on teardown."""
        try:
            # Add any request cleanup logic here
            if error:
                logger.error(f"Request error: {error}")
        except Exception as e:
            logger.error(f"Error in request cleanup: {e}")


def health_check_services(services):
    """Perform health check on all services."""
    health_status = {
        'overall': 'healthy',
        'services': {},
        'warnings': []
    }
    
    try:
        # Check document processor
        if services['document_processor_available']:
            health_status['services']['document_processor'] = 'ready'
        else:
            health_status['services']['document_processor'] = 'fallback'
            health_status['warnings'].append('Document processor using fallback implementation')
        
        # Check style analyzer
        if services['style_analyzer_available']:
            health_status['services']['style_analyzer'] = 'ready'
        else:
            health_status['services']['style_analyzer'] = 'fallback'
            health_status['warnings'].append('Style analyzer using fallback implementation')
        
        # Check AI rewriter
        if services['ai_rewriter_available']:
            health_status['services']['ai_rewriter'] = 'ready'
        else:
            health_status['services']['ai_rewriter'] = 'fallback'
            health_status['warnings'].append('AI rewriter using fallback implementation')
        
        # Check database service
        if services.get('database_available', False):
            health_status['services']['database'] = 'ready'
        else:
            health_status['services']['database'] = 'unavailable'
            health_status['warnings'].append('Database service unavailable - no data persistence')
        
        # Overall health assessment
        if len(health_status['warnings']) > 3:
            health_status['overall'] = 'degraded'
        
    except Exception as e:
        logger.error(f"Error in health check: {e}")
        health_status['overall'] = 'unhealthy'
        health_status['error'] = str(e)
    
    return health_status


def register_cleanup_handlers():
    """Register cleanup handlers for graceful shutdown."""
    
    def cleanup_ruby_client():
        """Cleanup the Ruby client on shutdown."""
        try:
            from structural_parsing.asciidoc.ruby_client import shutdown_client
            shutdown_client()
            logger.info("Ruby client shut down successfully")
        except Exception as e:
            logger.error(f"Error shutting down Ruby client: {e}")
    
    # Register cleanup on normal exit
    atexit.register(cleanup_ruby_client)
    
    # Register cleanup on signal termination
    def signal_handler(signum, frame):
        logger.info(f"Received signal {signum}, shutting down...")
        cleanup_ruby_client()
        exit(0)
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler) 