"""
Word Usage Rule for words starting with 'T' (Production-Grade)
Evidence-based analysis with surgical zero false positive guards for T-word usage detection.
Based on IBM Style Guide recommendations with production-grade evidence calculation.

CORRECTED "THAT" DETECTION: 
- Detects MISSING "that" after reporting verbs (verify, note, ensure, etc.)
- Provides clear suggestions to INSERT "that" for global audience clarity
- No longer incorrectly flags the presence of "that"
"""
from typing import List, Dict, Any
from .base_word_usage_rule import BaseWordUsageRule
import re

try:
    from spacy.tokens import Doc
except ImportError:
    Doc = None

class TWordsRule(BaseWordUsageRule):
    """
    PRODUCTION-GRADE: Checks for the incorrect usage of specific words starting with 'T'.
    
    Implements evidence-based analysis with:
    - Surgical zero false positive guards for T-word usage
    - Dynamic base evidence scoring based on word specificity and context
    - Context-aware adjustments for different writing domains
    
    Features:
    - Near 100% false positive elimination through surgical guards
    - Word-specific evidence calculation for each T-word violation
    - Evidence-aware suggestions tailored to writing context
    """
    def _get_rule_type(self) -> str:
        return 'word_usage_t'

    def analyze(self, text: str, sentences: List[str], nlp=None, context=None) -> List[Dict[str, Any]]:
        """
        Evidence-based analysis for T-word usage violations.
        Computes a nuanced evidence score per occurrence considering linguistic,
        structural, semantic, and feedback clues.
        """
        # === UNIVERSAL CODE CONTEXT GUARD ===
        # Skip analysis for code blocks, listings, and literal blocks (technical syntax, not prose)
        if context and context.get('block_type') in ['listing', 'literal', 'code_block', 'inline_code']:
            return []
        errors: List[Dict[str, Any]] = []
        if not nlp:
            return errors
            
        doc = nlp(text)
        
        # Define T-word patterns with evidence categories
        t_word_patterns = {
            "tap on": {"alternatives": ["tap"], "category": "redundant_preposition", "severity": "medium"},
            "tarball": {"alternatives": [".tar file"], "category": "jargon", "severity": "medium"},
            "team room": {"alternatives": ["teamroom"], "category": "spacing", "severity": "low"},
            "terminate": {"alternatives": ["end", "stop"], "category": "word_choice", "severity": "low"},
            "thank you": {"alternatives": ["(remove)"], "category": "cultural_sensitivity", "severity": "medium"},
            "time frame": {"alternatives": ["timeframe"], "category": "spacing", "severity": "low"},
            "time out": {"alternatives": ["time out (verb)", "timeout (noun)"], "category": "form_usage", "severity": "low"},
            "toast": {"alternatives": ["notification"], "category": "ui_language", "severity": "medium"},
            "tool kit": {"alternatives": ["toolkit"], "category": "spacing", "severity": "low"},
            "trade-off": {"alternatives": ["tradeoff"], "category": "hyphenation", "severity": "low"},
            "transparent": {"alternatives": ["clear", "obvious"], "category": "ambiguous_term", "severity": "medium"},
            "tribe": {"alternatives": ["team", "squad"], "category": "inclusive_language", "severity": "high"},
            "try and": {"alternatives": ["try to"], "category": "grammar", "severity": "medium"},
            "text": {"alternatives": [], "category": "acceptable_usage", "severity": "none"},
            "table": {"alternatives": [], "category": "acceptable_usage", "severity": "none"},
            "true": {"alternatives": [], "category": "acceptable_usage", "severity": "none"},
            "test": {"alternatives": [], "category": "acceptable_usage", "severity": "none"},
        }

        # Evidence-based analysis for T-words using lemma-based matching and phrase detection
        
        # 1. Single-word matches
        for token in doc:
            # Check if token lemma matches any of our target words
            token_lemma = token.lemma_.lower()
            token_text = token.text.lower()
            matched_pattern = None
            
            # Check single words (excluding multi-word patterns) - case-insensitive
            for pattern in t_word_patterns:
                if ' ' not in pattern and '-' not in pattern:  # Single word pattern (no hyphens/spaces)
                    if (token_lemma == pattern.lower() or 
                        token_text == pattern.lower()):
                        matched_pattern = pattern
                        break
            
            if matched_pattern:
                details = t_word_patterns[matched_pattern]
                
                # Skip acceptable usage patterns
                if details["category"] == "acceptable_usage":
                    continue
                
                # Apply surgical guards
                if self._apply_surgical_zero_false_positive_guards_word_usage(token, context or {}):
                    continue
                
                sent = token.sent
                sentence_index = 0
                for i, s in enumerate(doc.sents):
                    if s == sent:
                        sentence_index = i
                        break
                
                evidence_score = self._calculate_t_word_evidence(
                    matched_pattern, token, sent, text, context or {}, details["category"]
                )
                
                if evidence_score > 0.1:
                    errors.append(self._create_error(
                        sentence=sent.text,
                        sentence_index=sentence_index,
                        message=self._generate_evidence_aware_word_usage_message(matched_pattern, evidence_score, details["category"]),
                        suggestions=self._generate_evidence_aware_word_usage_suggestions(matched_pattern, details["alternatives"], evidence_score, context or {}, details["category"]),
                        severity=details["severity"] if evidence_score < 0.7 else 'high',
                        text=text,
                        context=context,
                        evidence_score=evidence_score,
                        span=(token.idx, token.idx + len(token.text)),
                        flagged_text=token.text
                    ))

        # 2. Multi-word phrase detection for T-words (including hyphenated words)
        multi_word_patterns = {pattern: details for pattern, details in t_word_patterns.items() if (' ' in pattern or '-' in pattern) and details["category"] != "acceptable_usage"}
        
        if multi_word_patterns:
            phrase_matches = self._find_multi_word_phrases_with_lemma(doc, list(multi_word_patterns.keys()), case_sensitive=False)
            
            for match in phrase_matches:
                pattern = match['phrase']
                details = multi_word_patterns[pattern]
                
                # Apply surgical guards on the first token
                if self._apply_surgical_zero_false_positive_guards_word_usage(match['start_token'], context or {}):
                    continue
                
                sent = match['start_token'].sent
                sentence_index = 0
                for i, s in enumerate(doc.sents):
                    if s == sent:
                        sentence_index = i
                        break
                
                evidence_score = self._calculate_t_word_evidence(
                    pattern, match['start_token'], sent, text, context or {}, details["category"]
                )
                
                if evidence_score > 0.1:
                    errors.append(self._create_error(
                        sentence=sent.text,
                        sentence_index=sentence_index,
                        message=self._generate_evidence_aware_word_usage_message(pattern, evidence_score, details["category"]),
                        suggestions=self._generate_evidence_aware_word_usage_suggestions(pattern, details["alternatives"], evidence_score, context or {}, details["category"]),
                        severity=details["severity"] if evidence_score < 0.7 else 'high',
                        text=text,
                        context=context,
                        evidence_score=evidence_score,
                        span=(match['start_char'], match['end_char']),
                        flagged_text=match['actual_text']
                    ))

        # 3. Special handling for hyphenated T-words
        # Handle patterns like "trade-off" that are tokenized as ["word", "-", "word"]
        hyphenated_patterns = {
            'trade-off': {'alternatives': ['tradeoff'], 'category': 'hyphenation', 'severity': 'low'}
        }
        
        for i in range(len(doc) - 2):
            token1 = doc[i]
            hyphen = doc[i + 1]
            token2 = doc[i + 2]
            
            if hyphen.text == '-':
                # Check if this forms a hyphenated pattern we're looking for
                combined_text = f"{token1.text.lower()}-{token2.text.lower()}"
                
                if combined_text in hyphenated_patterns:
                    details = hyphenated_patterns[combined_text]
                    
                    # Apply surgical guards on the first token
                    if self._apply_surgical_zero_false_positive_guards_word_usage(token1, context or {}):
                        continue
                    
                    sent = token1.sent
                    sentence_index = 0
                    for j, s in enumerate(doc.sents):
                        if s == sent:
                            sentence_index = j
                            break
                    
                    evidence_score = self._calculate_t_word_evidence(
                        combined_text, token1, sent, text, context or {}, details["category"]
                    )
                    
                    if evidence_score > 0.1:
                        errors.append(self._create_error(
                            sentence=sent.text,
                            sentence_index=sentence_index,
                            message=self._generate_evidence_aware_word_usage_message(combined_text, evidence_score, details["category"]),
                            suggestions=self._generate_evidence_aware_word_usage_suggestions(combined_text, details["alternatives"], evidence_score, context or {}, details["category"]),
                            severity=details["severity"] if evidence_score < 0.7 else 'high',
                            text=text,
                            context=context,
                            evidence_score=evidence_score,
                            span=(token1.idx, token2.idx + len(token2.text)),
                            flagged_text=f"{token1.text}-{token2.text}"
                        ))

        # 4. Special detection for missing "that" after reporting verbs
        missing_that_errors = self._detect_missing_that_violations(doc, text, context or {})
        errors.extend(missing_that_errors)
        
        return errors

    def _calculate_t_word_evidence(self, word: str, token, sentence, text: str, context: Dict[str, Any], category: str) -> float:
        """
        PRODUCTION-GRADE: Calculate evidence score (0.0-1.0) for T-word usage violations.
        
        Implements rule-specific evidence calculation with:
        - Dynamic base evidence scoring based on word category and violation type
        - Context-aware adjustments for different writing domains
        - Linguistic, structural, semantic, and feedback pattern analysis
        - Special handling for inclusive language and cultural sensitivity
        
        Args:
            word: The T-word being analyzed
            token: SpaCy token object
            sentence: Sentence containing the word
            text: Full document text
            context: Document context (block_type, content_type, etc.)
            category: Word category (inclusive_language, cultural_sensitivity, jargon, etc.)
            
        Returns:
            float: Evidence score from 0.0 (no evidence) to 1.0 (strong evidence)
        """
        
        # === DYNAMIC BASE EVIDENCE ASSESSMENT ===
        evidence_score = self._get_base_t_word_evidence_score(word, category, sentence, context)
        
        if evidence_score == 0.0:
            return 0.0  # No evidence, skip this word
        
        # === STEP 1: LINGUISTIC CLUES (MICRO-LEVEL) ===
        evidence_score = self._apply_linguistic_clues_t_words(evidence_score, word, token, sentence)
        
        # === STEP 2: STRUCTURAL CLUES (MESO-LEVEL) ===
        evidence_score = self._apply_structural_clues_t_words(evidence_score, context)
        
        # === STEP 3: SEMANTIC CLUES (MACRO-LEVEL) ===
        evidence_score = self._apply_semantic_clues_t_words(evidence_score, word, text, context)
        
        # === STEP 4: FEEDBACK PATTERNS (LEARNING CLUES) ===
        evidence_score = self._apply_feedback_clues_t_words(evidence_score, word, context)
        
        return max(0.0, min(1.0, evidence_score))  # Clamp to valid range
    
    def _get_base_t_word_evidence_score(self, word: str, category: str, sentence, context: Dict[str, Any]) -> float:
        """
        Set dynamic base evidence score based on T-word category and violation specificity.
        Higher priority categories get higher base scores for surgical precision.
        """
        word_lower = word.lower()
        
        # Very high-risk inclusive language issues
        if category == 'inclusive_language':
            if word_lower == 'tribe':
                return 0.85  # Critical inclusive language violation
            else:
                return 0.85  # Other inclusive language issues
        
        # High-risk professionalism and clarity issues
        elif category in ['jargon', 'ui_language', 'cultural_sensitivity']:
            if word_lower == 'tarball':
                return 0.75  # Technical jargon clarity
            elif word_lower == 'toast':
                return 0.7   # UI terminology professionalism
            elif word_lower == 'thank you':
                return 0.8   # Cultural sensitivity critical
            else:
                return 0.75  # Other professionalism issues
        
        # Medium-high risk clarity and correctness issues
        elif category in ['redundant_preposition', 'grammar', 'ambiguous_term']:
            if word_lower == 'tap on':
                return 0.65  # Redundant preposition clarity
            elif word_lower == 'try and':
                return 0.7   # Grammar correctness important
            elif word_lower == 'transparent':
                return 0.65  # Ambiguous term clarity
            else:
                return 0.7   # Other clarity issues
        
        # Medium-risk improvement opportunities
        elif category in ['word_choice', 'form_usage', 'clarity']:
            if word_lower == 'terminate':
                return 0.55  # Word choice context-dependent
            elif word_lower == 'time out':
                return 0.6   # Form usage important
            else:
                return 0.6   # Other improvement opportunities
        
        # Lower-risk consistency issues
        elif category in ['spacing', 'hyphenation']:
            if word_lower in ['team room', 'time frame', 'tool kit']:
                return 0.45  # Spacing consistency
            elif word_lower == 'trade-off':
                return 0.5   # Hyphenation consistency
            else:
                return 0.5   # Other consistency issues
        
        return 0.6  # Default moderate evidence for other patterns

    def _apply_linguistic_clues_t_words(self, ev: float, word: str, token, sentence) -> float:
        """Apply T-word-specific linguistic clues using SpaCy analysis."""
        sent_text = sentence.text.lower()
        word_lower = word.lower()
        
        # === INCLUSIVE LANGUAGE CLUES ===
        if word_lower == 'tribe':
            # Organizational context needs inclusive language
            if any(indicator in sent_text for indicator in ['team', 'organization', 'group', 'department']):
                ev += 0.3  # Strong organizational context suggests inclusive language need
            elif any(indicator in sent_text for indicator in ['work', 'project', 'collaboration', 'members']):
                ev += 0.2  # Work context benefits from inclusive language
            elif any(indicator in sent_text for indicator in ['culture', 'values', 'community']):
                ev += 0.15  # Cultural context needs sensitivity
        
        # === CULTURAL SENSITIVITY CLUES ===
        if word_lower == 'thank you':
            # Technical content should avoid cultural assumptions
            if any(indicator in sent_text for indicator in ['technical', 'documentation', 'guide', 'manual']):
                ev += 0.2  # Technical content should be culturally neutral
            elif any(indicator in sent_text for indicator in ['instruction', 'procedure', 'step']):
                ev += 0.15  # Procedural content benefits from objectivity
            elif any(indicator in sent_text for indicator in ['international', 'global', 'worldwide']):
                ev += 0.25  # International content needs cultural neutrality
        
        # === UI LANGUAGE CLUES ===
        if word_lower == 'toast':
            # UI context needs professional terminology
            if any(indicator in sent_text for indicator in ['ui', 'interface', 'notification', 'alert']):
                ev += 0.15  # UI context needs professional terminology
            elif any(indicator in sent_text for indicator in ['display', 'show', 'appear', 'popup']):
                ev += 0.1  # Display context benefits from precise terminology
            elif any(indicator in sent_text for indicator in ['user', 'customer', 'client']):
                ev += 0.05  # User-facing context benefits from clear language
        
        # === GRAMMAR CLUES ===
        if word_lower == 'try and':
            # Action context benefits from correct grammar
            if any(indicator in sent_text for indicator in ['attempt', 'effort', 'action', 'goal']):
                ev += 0.15  # Action context benefits from correct grammar
            elif any(indicator in sent_text for indicator in ['procedure', 'instruction', 'step']):
                ev += 0.1  # Procedural context needs grammatical precision
            # Check grammatical context using POS tags
            if hasattr(token, 'head') and token.head.pos_ in ['VERB']:
                if token.head.lemma_.lower() in ['will', 'should', 'must', 'can']:
                    ev += 0.05  # Modal context suggests infinitive usage
        
        # === JARGON CLUES ===
        if word_lower == 'tarball':
            # Technical jargon clarity for broader audiences
            if any(indicator in sent_text for indicator in ['user', 'customer', 'documentation', 'guide']):
                ev += 0.2  # User-facing content needs accessible language
            elif any(indicator in sent_text for indicator in ['download', 'install', 'extract']):
                ev += 0.15  # User action context benefits from clear terminology
            elif any(indicator in sent_text for indicator in ['file', 'archive', 'package']):
                ev += 0.1  # File context can benefit from standard terminology
        
        # === REDUNDANT PREPOSITION CLUES ===
        if word_lower == 'tap on':
            # Redundancy context
            if any(indicator in sent_text for indicator in ['button', 'link', 'icon', 'menu']):
                ev += 0.1  # UI element context benefits from concise language
            elif any(indicator in sent_text for indicator in ['click', 'select', 'touch']):
                ev += 0.05  # Action context can be more concise
        
        # === AMBIGUOUS TERM CLUES ===
        if word_lower == 'transparent':
            # Ambiguity context
            if any(indicator in sent_text for indicator in ['process', 'operation', 'system']):
                ev += 0.15  # Technical context needs precise language
            elif any(indicator in sent_text for indicator in ['user', 'visible', 'obvious']):
                ev += 0.1  # User experience context benefits from clarity
        
        # === WORD CHOICE CLUES ===
        if word_lower == 'terminate':
            # Context-dependent word choice
            if any(indicator in sent_text for indicator in ['user', 'customer', 'person']):
                ev += 0.15  # People context benefits from gentler language
            elif any(indicator in sent_text for indicator in ['process', 'session', 'connection']):
                ev += 0.05  # Technical context may accept technical terms
        
        return ev

    def _apply_structural_clues_t_words(self, ev: float, context: Dict[str, Any]) -> float:
        """Apply structural context clues for T-words."""
        block_type = context.get('block_type', 'paragraph')
        if block_type in ['step', 'procedure']:
            ev += 0.1
        elif block_type == 'heading':
            ev -= 0.1
        return ev

    def _apply_semantic_clues_t_words(self, ev: float, word: str, text: str, context: Dict[str, Any]) -> float:
        """Apply semantic and content-type clues for T-words."""
        content_type = context.get('content_type', 'general')
        audience = context.get('audience', 'general')
        word_lower = word.lower()
        
        if content_type == 'customer_facing' and word_lower in ['tribe', 'tarball', 'thank you']:
            ev += 0.25  # Customer content needs professional, inclusive language
        elif content_type == 'international' and word_lower in ['tribe', 'thank you']:
            ev += 0.2  # International content requires inclusive, neutral language
        elif content_type == 'technical' and word_lower in ['tarball', 'terminate']:
            ev += 0.15  # Technical docs benefit from clear, standard terminology
        elif content_type == 'ui_documentation' and word_lower == 'toast':
            ev += 0.2  # UI docs need professional interface terminology
        
        if audience == 'external' and word_lower in ['tribe', 'tarball']:
            ev += 0.2  # External audiences need clear, professional language
        elif audience == 'global' and word_lower in ['tribe', 'thank you']:
            ev += 0.15  # Global audiences need inclusive, culturally neutral language
        
        return ev

    def _apply_feedback_clues_t_words(self, ev: float, word: str, context: Dict[str, Any]) -> float:
        """Apply feedback pattern clues for T-words."""
        patterns = self._get_cached_feedback_patterns_t_words()
        word_lower = word.lower()
        
        # Consistently flagged terms
        if word_lower in patterns.get('often_flagged_terms', set()):
            ev += 0.1
        
        # Consistently accepted terms
        if word_lower in patterns.get('accepted_terms', set()):
            ev -= 0.2
        
        # Context-specific patterns
        content_type = context.get('content_type', 'general')
        context_patterns = patterns.get(f'{content_type}_patterns', {})
        
        if word_lower in context_patterns.get('flagged', set()):
            ev += 0.1
        elif word_lower in context_patterns.get('accepted', set()):
            ev -= 0.25  # Strong reduction for context-appropriate terms
        
        return ev

    def _get_cached_feedback_patterns_t_words(self) -> Dict[str, Any]:
        """Get cached feedback patterns for T-words."""
        return {
            'often_flagged_terms': {'tribe', 'toast', 'thank you', 'try and', 'tarball', 'transparent'},
            'accepted_terms': {'text', 'table', 'true', 'test', 'terminate'},  # Generally acceptable terms in some contexts
            'technical_patterns': {
                'flagged': {'tribe', 'toast', 'thank you'},  # Technical docs need professional language
                'accepted': {'terminate', 'time out', 'tarball', 'text', 'table'}  # Technical terms acceptable
            },
            'customer_facing_patterns': {
                'flagged': {'tribe', 'tarball', 'thank you', 'toast', 'terminate'},  # Customer content needs accessible language
                'accepted': {'text', 'table', 'true', 'test'}  # Customer-friendly terms
            },
            'ui_documentation_patterns': {
                'flagged': {'toast', 'tap on', 'tribe'},  # UI docs need professional terminology
                'accepted': {'text', 'table', 'time out', 'test'}  # UI context terms
            },
            'international_patterns': {
                'flagged': {'tribe', 'thank you', 'transparent'},  # International content needs cultural neutrality
                'accepted': {'text', 'table', 'true', 'test'}  # Neutral terms
            },
            'documentation_patterns': {
                'flagged': {'tribe', 'thank you', 'tarball', 'try and'},  # Documentation needs clear language
                'accepted': {'text', 'table', 'test', 'terminate'}  # Documentation-friendly terms
            },
            'formal_patterns': {
                'flagged': {'toast', 'try and', 'tribe'},  # Formal writing prefers precise language
                'accepted': {'terminate', 'text', 'table', 'true'}  # Formal terms acceptable
            },
            'general_patterns': {
                'flagged': {'tribe', 'toast', 'thank you'},  # General content prefers inclusive language
                'accepted': {'text', 'table', 'true', 'test', 'terminate'}  # Common terms acceptable
            }
        }

    def _detect_missing_that_violations(self, doc, text: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect missing "that" after reporting verbs for clarity.
        
        Looks for patterns like:
        - "Verify the system is running" → should be "Verify that the system is running"
        - "Note these changes will have..." → should be "Note that these changes will have..."
        
        Returns list of error dictionaries for missing "that" violations.
        """
        errors = []
        
        # Define reporting verbs that often need "that" for clarity
        reporting_verbs = {
            'verify', 'ensure', 'confirm', 'check', 'note', 'observe', 'notice',
            'realize', 'understand', 'recognize', 'acknowledge', 'assume',
            'believe', 'suppose', 'expect', 'remember', 'forget', 'know',
            'see', 'find', 'discover', 'learn', 'hear', 'feel', 'think'
        }
        
        for sent_idx, sent in enumerate(doc.sents):
            # Look for reporting verbs in this sentence
            for token in sent:
                if (hasattr(token, 'lemma_') and token.lemma_.lower() in reporting_verbs and
                    hasattr(token, 'pos_') and token.pos_ == 'VERB'):
                    
                    # Check if this verb is followed by a noun phrase without "that"
                    violation_detected, insertion_point, noun_phrase = self._check_missing_that_pattern(token)
                    
                    if violation_detected:
                        # Apply surgical guards
                        if self._apply_surgical_zero_false_positive_guards_word_usage(token, context):
                            continue
                        
                        # Calculate evidence score for missing "that"
                        evidence_score = self._calculate_missing_that_evidence(
                            token, sent, text, context
                        )
                        
                        if evidence_score > 0.1:
                            # Create clear, actionable suggestion
                            verb_text = token.text
                            suggested_text = f"For clarity, consider inserting 'that' after '{verb_text}'"
                            
                            errors.append(self._create_error(
                                sentence=sent.text,
                                sentence_index=sent_idx,
                                message=f"Missing 'that' for clarity after '{verb_text}'",
                                suggestions=[suggested_text],
                                severity='low' if evidence_score < 0.7 else 'medium',
                                text=text,
                                context=context,
                                evidence_score=evidence_score,
                                span=(insertion_point, insertion_point),  # Point where "that" should be inserted
                                flagged_text=f"{verb_text} {noun_phrase}",
                                violation_type='missing_that'
                            ))
        
        return errors
    
    def _check_missing_that_pattern(self, verb_token):
        """
        Check if a reporting verb is followed by a noun phrase without "that".
        
        Returns:
            tuple: (violation_detected: bool, insertion_point: int, noun_phrase: str)
        """
        # Look at the tokens immediately following the verb
        sent = verb_token.sent
        verb_idx = verb_token.i - sent.start
        sent_tokens = list(sent)
        
        # Skip if this is the last token in the sentence
        if verb_idx >= len(sent_tokens) - 1:
            return False, 0, ""
        
        next_token = sent_tokens[verb_idx + 1]
        
        # Skip if the next token is "that" (already has it)
        if hasattr(next_token, 'text') and next_token.text.lower() == 'that':
            return False, 0, ""
        
        # Skip if there's a direct object pronoun (me, you, him, her, it, us, them)
        if (hasattr(next_token, 'text') and 
            next_token.text.lower() in ['me', 'you', 'him', 'her', 'it', 'us', 'them']):
            return False, 0, ""
        
        # Skip if followed by an infinitive (to + verb)
        if (hasattr(next_token, 'text') and next_token.text.lower() == 'to' and
            verb_idx < len(sent_tokens) - 2 and
            hasattr(sent_tokens[verb_idx + 2], 'pos_') and 
            sent_tokens[verb_idx + 2].pos_ == 'VERB'):
            return False, 0, ""
        
        # Look for a noun phrase pattern that suggests missing "that"
        # Pattern: reporting_verb + determiner/noun + ...
        if (hasattr(next_token, 'pos_') and 
            next_token.pos_ in ['DET', 'NOUN', 'PRON', 'PROPN', 'ADJ'] and
            verb_idx < len(sent_tokens) - 2):
            
            # Check if this looks like a clause (has a verb later)
            has_clause_verb = False
            noun_phrase_tokens = []
            
            for i in range(verb_idx + 1, min(verb_idx + 6, len(sent_tokens))):  # Look ahead up to 5 tokens
                token = sent_tokens[i]
                noun_phrase_tokens.append(token.text)
                
                if hasattr(token, 'pos_') and token.pos_ == 'VERB' and token.i != verb_token.i:
                    has_clause_verb = True
                    break
            
            if has_clause_verb and len(noun_phrase_tokens) >= 2:
                insertion_point = next_token.idx  # Character position where "that" should go
                noun_phrase = ' '.join(noun_phrase_tokens[:3])  # First few tokens for context
                return True, insertion_point, noun_phrase
        
        return False, 0, ""
    
    def _calculate_missing_that_evidence(self, verb_token, sentence, text: str, context: Dict[str, Any]) -> float:
        """
        Calculate evidence score for missing "that" violations.
        
        Higher scores indicate stronger evidence that "that" should be inserted for clarity.
        """
        # Base evidence score - missing "that" is generally a clarity issue
        evidence_score = 0.6
        
        # Higher evidence for certain reporting verbs that especially benefit from "that"
        verb_lemma = verb_token.lemma_.lower()
        high_clarity_verbs = {'note', 'observe', 'verify', 'confirm', 'ensure'}
        if verb_lemma in high_clarity_verbs:
            evidence_score += 0.2
        
        # Higher evidence for complex or long noun phrases
        sent_text = sentence.text.lower()
        if len(sent_text.split()) > 10:  # Longer sentences benefit more from "that"
            evidence_score += 0.1
        
        # Content type adjustments
        content_type = context.get('content_type', 'general')
        if content_type in ['documentation', 'technical', 'international']:
            evidence_score += 0.15  # Technical/international content needs clarity
        elif content_type in ['user_guide', 'tutorial']:
            evidence_score += 0.1   # User-facing content benefits from clarity
        
        # Audience adjustments
        audience = context.get('audience', 'general')
        if audience in ['global', 'international', 'external']:
            evidence_score += 0.15  # Global audiences especially benefit from "that"
        
        # Structural context adjustments
        block_type = context.get('block_type', 'paragraph')
        if block_type in ['step', 'procedure', 'instruction']:
            evidence_score += 0.1  # Procedural content needs clarity
        
        return max(0.0, min(1.0, evidence_score))